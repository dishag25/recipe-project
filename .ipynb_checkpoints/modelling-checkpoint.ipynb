{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212255, 28)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data =pd.read_pickle(\"food.pkl\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>...</th>\n",
       "      <th>submitted_month</th>\n",
       "      <th>submitted_year</th>\n",
       "      <th>dairy-free</th>\n",
       "      <th>gluten-free</th>\n",
       "      <th>low-carb</th>\n",
       "      <th>vegan</th>\n",
       "      <th>vegetarian</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168477</th>\n",
       "      <td>sesame dipping sauce</td>\n",
       "      <td>353233</td>\n",
       "      <td>10</td>\n",
       "      <td>406741</td>\n",
       "      <td>2009-01-31</td>\n",
       "      <td>['lactose', '15-minutes-or-less', 'time-to-mak...</td>\n",
       "      <td>2</td>\n",
       "      <td>['in a small bowl , whisk together soy sauce ,...</td>\n",
       "      <td>this works well for dipping pot stickers in as...</td>\n",
       "      <td>['soy sauce', 'rice vinegar', 'sesame oil', 'a...</td>\n",
       "      <td>...</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>353233</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201337</th>\n",
       "      <td>unattended rib roast</td>\n",
       "      <td>97973</td>\n",
       "      <td>112</td>\n",
       "      <td>152393</td>\n",
       "      <td>2004-08-17</td>\n",
       "      <td>['time-to-make', 'main-ingredient', 'preparati...</td>\n",
       "      <td>6</td>\n",
       "      <td>['at noon , preheat oven to 375 degrees', 'sea...</td>\n",
       "      <td>i found this recipe in the paper and thought i...</td>\n",
       "      <td>['standing rib roast', 'salt and pepper']</td>\n",
       "      <td>...</td>\n",
       "      <td>Aug</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>97973</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173114</th>\n",
       "      <td>slow cooker mashed potatoes</td>\n",
       "      <td>265017</td>\n",
       "      <td>135</td>\n",
       "      <td>290107</td>\n",
       "      <td>2007-11-12</td>\n",
       "      <td>['time-to-make', 'course', 'main-ingredient', ...</td>\n",
       "      <td>6</td>\n",
       "      <td>['in a mixing bowl , combine cream cheese , so...</td>\n",
       "      <td>this recipe is from taste of home magazine. it...</td>\n",
       "      <td>['cream cheese', 'sour cream', 'butter', 'drie...</td>\n",
       "      <td>...</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>265017</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29931</th>\n",
       "      <td>butterscotch toffee cookies</td>\n",
       "      <td>288112</td>\n",
       "      <td>25</td>\n",
       "      <td>668077</td>\n",
       "      <td>2008-02-23</td>\n",
       "      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>8</td>\n",
       "      <td>['mix shortening and both sugars with electric...</td>\n",
       "      <td>in preparation for a bake sale, i changed a re...</td>\n",
       "      <td>['shortening', 'sugar', 'brown sugar', 'egg', ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>288112</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9300</th>\n",
       "      <td>au gratin hash browns casserole</td>\n",
       "      <td>94740</td>\n",
       "      <td>70</td>\n",
       "      <td>126418</td>\n",
       "      <td>2004-06-30</td>\n",
       "      <td>['time-to-make', 'course', 'preparation', 'cas...</td>\n",
       "      <td>6</td>\n",
       "      <td>['oven@ 350', 'in a large bowl combine first 5...</td>\n",
       "      <td>i think i found this recipe in my \"goody bag\" ...</td>\n",
       "      <td>['cream of chicken soup', 'sour cream', 'marga...</td>\n",
       "      <td>...</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94740</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name      id  minutes contributor_id  \\\n",
       "168477             sesame dipping sauce  353233       10         406741   \n",
       "201337             unattended rib roast   97973      112         152393   \n",
       "173114      slow cooker mashed potatoes  265017      135         290107   \n",
       "29931       butterscotch toffee cookies  288112       25         668077   \n",
       "9300    au gratin hash browns casserole   94740       70         126418   \n",
       "\n",
       "        submitted                                               tags  n_steps  \\\n",
       "168477 2009-01-31  ['lactose', '15-minutes-or-less', 'time-to-mak...        2   \n",
       "201337 2004-08-17  ['time-to-make', 'main-ingredient', 'preparati...        6   \n",
       "173114 2007-11-12  ['time-to-make', 'course', 'main-ingredient', ...        6   \n",
       "29931  2008-02-23  ['30-minutes-or-less', 'time-to-make', 'course...        8   \n",
       "9300   2004-06-30  ['time-to-make', 'course', 'preparation', 'cas...        6   \n",
       "\n",
       "                                                    steps  \\\n",
       "168477  ['in a small bowl , whisk together soy sauce ,...   \n",
       "201337  ['at noon , preheat oven to 375 degrees', 'sea...   \n",
       "173114  ['in a mixing bowl , combine cream cheese , so...   \n",
       "29931   ['mix shortening and both sugars with electric...   \n",
       "9300    ['oven@ 350', 'in a large bowl combine first 5...   \n",
       "\n",
       "                                              description  \\\n",
       "168477  this works well for dipping pot stickers in as...   \n",
       "201337  i found this recipe in the paper and thought i...   \n",
       "173114  this recipe is from taste of home magazine. it...   \n",
       "29931   in preparation for a bake sale, i changed a re...   \n",
       "9300    i think i found this recipe in my \"goody bag\" ...   \n",
       "\n",
       "                                              ingredients  ...  \\\n",
       "168477  ['soy sauce', 'rice vinegar', 'sesame oil', 'a...  ...   \n",
       "201337          ['standing rib roast', 'salt and pepper']  ...   \n",
       "173114  ['cream cheese', 'sour cream', 'butter', 'drie...  ...   \n",
       "29931   ['shortening', 'sugar', 'brown sugar', 'egg', ...  ...   \n",
       "9300    ['cream of chicken soup', 'sour cream', 'marga...  ...   \n",
       "\n",
       "        submitted_month  submitted_year  dairy-free  gluten-free  low-carb  \\\n",
       "168477              Jan            2009           0            0         0   \n",
       "201337              Aug            2004           0            0         1   \n",
       "173114              Nov            2007           0            0         0   \n",
       "29931               Feb            2008           0            0         0   \n",
       "9300                Jun            2004           0            0         0   \n",
       "\n",
       "        vegan  vegetarian  recipe_id average_rating  votes  \n",
       "168477      1           1     353233            5.0      2  \n",
       "201337      0           0      97973            5.0      1  \n",
       "173114      0           0     265017            5.0      2  \n",
       "29931       0           0     288112            4.0      1  \n",
       "9300        0           0      94740            5.0      1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sampled Dataset: (53064, 28)\n"
     ]
    }
   ],
   "source": [
    "# Sample 25% of the dataset\n",
    "sampled_data = data.sample(frac=0.25, random_state=42)\n",
    "\n",
    "# View sample\n",
    "display(sampled_data.head())\n",
    "\n",
    "# Print the shape of the sampled dataset\n",
    "print(\"Shape of Sampled Dataset:\", sampled_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kelly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for tokenizer\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def recipe_tokenizer(sentence):\n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "\n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for word embedding using Word2Vec\n",
    "def word_embedding(sampled_data, column):\n",
    "    # Tokenize the text data\n",
    "    tokenized_data = sampled_data[column].apply(recipe_tokenizer)\n",
    "\n",
    "    # Train a Word2Vec model\n",
    "    model = Word2Vec(tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Create word embeddings for each word in the vocabulary\n",
    "    embeddings = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pre-compute and store the combined embeddings\n",
    "def precompute_embeddings(sampled_data):\n",
    "    # Step 1: Process 'ingredients' using word2vec and create word embeddings\n",
    "    embeddings = word_embedding(sampled_data, 'ingredients')\n",
    "\n",
    "    # Step 2: Concatenate relevant columns (excluding 'ingredients')\n",
    "    sampled_data['text_data'] = sampled_data[['name', 'tags', 'description']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    # Step 3: Preprocess the text data (example: lowercase conversion)\n",
    "    sampled_data['text_data'] = sampled_data['text_data'].str.lower()\n",
    "\n",
    "    # Step 4: Vectorize the text data (excluding 'ingredients') using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(min_df=5,\n",
    "                                 tokenizer=recipe_tokenizer)\n",
    "    vectorized_data = vectorizer.fit_transform(sampled_data['text_data'])\n",
    "\n",
    "    # Step 5: Retrieve the word embeddings for 'ingredients'\n",
    "    ingredient_embeddings = [np.mean([embeddings[word] for word in recipe_tokenizer(ingredients) if word in embeddings]\n",
    "                                      or [np.zeros(100)], axis=0) for ingredients in sampled_data['ingredients']]\n",
    "\n",
    "    # Step 6: Combine the vectorized data and ingredient embeddings\n",
    "    combined_embeddings = np.concatenate([vectorized_data.toarray(), np.array(ingredient_embeddings)], axis=1)\n",
    "    \n",
    "    # Step 7: Store combined embeddings in pkl file\n",
    "    with open('combined_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(combined_embeddings, f)\n",
    "    \n",
    "    # Step 8: Store the trained TF-IDF vectorizer model in a separate pkl file\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    \n",
    "    # Step 9: Done!  \n",
    "    print(\"Text data and TF-IDF vectorizer model stored in pkl files!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data and TF-IDF vectorizer model stored in pkl files!\n"
     ]
    }
   ],
   "source": [
    "# store vectorized data and the trained TF-IDF vectorizer model from sampled data\n",
    "precompute_embeddings(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the combined embeddings and TF-IDF vectorizer model\n",
    "def load_embeddings_and_vectorizer():\n",
    "    with open('combined_embeddings.pkl', 'rb') as f:\n",
    "        combined_embeddings = pickle.load(f)\n",
    "    with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    return combined_embeddings, vectorizer\n",
    "\n",
    "# Function for finding recipes\n",
    "def find_similar_recipes(sampled_data, user_input, num_similar=5):\n",
    "    try:\n",
    "        combined_embeddings, vectorizer = load_embeddings_and_vectorizer()\n",
    "    except FileNotFoundError:\n",
    "        precompute_embeddings(sampled_data)\n",
    "        combined_embeddings, vectorizer = load_embeddings_and_vectorizer()\n",
    "\n",
    "    # Process user input\n",
    "    # Create a DataFrame for user input\n",
    "    user_data = pd.DataFrame({'text_data': [user_input]})\n",
    "    user_data['text_data'] = user_data['text_data'].str.lower()\n",
    "\n",
    "    # Vectorize the user input using the provided vectorizer\n",
    "    user_vectorized_data = vectorizer.transform(user_data['text_data'])\n",
    "\n",
    "    # Ensure the number of features in user_vectorized_data matches with combined_embeddings\n",
    "    num_missing_features = combined_embeddings.shape[1] - user_vectorized_data.shape[1]\n",
    "    if num_missing_features > 0:\n",
    "        # Add zero columns to user_vectorized_data to match the number of features\n",
    "        user_vectorized_data = np.pad(user_vectorized_data.toarray(), ((0, 0), (0, num_missing_features)))\n",
    "\n",
    "    # Compute cosine similarity with user input\n",
    "    cosine_sim_matrix = cosine_similarity(user_vectorized_data, combined_embeddings)\n",
    "\n",
    "    # Retrieve similar recipe indices\n",
    "    similar_recipes = cosine_sim_matrix[0].argsort()[::-1][:num_similar]\n",
    "\n",
    "    # Get similar recipe names from food_df\n",
    "    similar_recipe_names = sampled_data.iloc[similar_recipes]['name'].tolist()\n",
    "\n",
    "    return similar_recipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['japanese noodle soup',\n",
       " 'okonomiyaki',\n",
       " 'vegetarian yakisoba',\n",
       " 'omuraisu  japanese omelette',\n",
       " 'japanese salad with ginger soy dressing']"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "find_similar_recipes(sampled_data, \"japanese dishes vegetarian\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
